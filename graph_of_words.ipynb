{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "from multiprocessing import Pool\n",
    "import igraph as ig\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda if torch.cuda.is_available() else \"Not available\")\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA is not available. Check your PyTorch installation and GPU setup.\")\n",
    "\n",
    "\n",
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub(r'http\\S+|www\\S+|[^\\w\\s]|[\\d+]', '', text.lower())\n",
    "    words = text.split()  # Tokenisation\n",
    "    words = [word for word in words if word not in stop_words]  # Suppression des stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def tweet_to_igraph(tweet):\n",
    "    words = tweet.split()\n",
    "    if not words:  # Vérifiez si le tweet est vide\n",
    "        return ig.Graph(directed=False)\n",
    "\n",
    "    graph = ig.Graph(directed=False)\n",
    "    \n",
    "    # Ajouter des nœuds\n",
    "    unique_words = list(set(words))\n",
    "    graph.add_vertices(unique_words)\n",
    "    \n",
    "    # Ajouter des arêtes avec des poids\n",
    "    edge_weights = {}\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            if i != j:  # Éviter les boucles\n",
    "                edge = tuple(sorted([word1, word2]))\n",
    "                edge_weights[edge] = edge_weights.get(edge, 0) + 1\n",
    "\n",
    "    edges, weights = zip(*edge_weights.items()) if edge_weights else ([], [])\n",
    "    graph.add_edges(edges)\n",
    "    graph.es['weight'] = weights\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "def combined_igraph_each_period(tweets):\n",
    "    combined_graph = ig.Graph(directed=False)\n",
    "    \n",
    "    # Ajout progressif des tweets au graphe combiné\n",
    "    for tweet in tweets:\n",
    "        tweet_graph = tweet_to_igraph(tweet)\n",
    "        \n",
    "        # Ajouter les nœuds et les arêtes du tweet au graphe combiné\n",
    "        new_vertices = set(tweet_graph.vs['name']) - set(combined_graph.vs['name'])\n",
    "        combined_graph.add_vertices(list(new_vertices))\n",
    "        \n",
    "        for edge, weight in zip(tweet_graph.get_edgelist(), tweet_graph.es['weight']):\n",
    "            if combined_graph.are_connected(*edge):\n",
    "                # Incrémentez le poids si l'arête existe déjà\n",
    "                eid = combined_graph.get_eid(*edge)\n",
    "                combined_graph.es[eid]['weight'] += weight\n",
    "            else:\n",
    "                # Sinon, ajoutez une nouvelle arête\n",
    "                combined_graph.add_edge(*edge, weight=weight)\n",
    "    \n",
    "    return combined_graph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_period_with_label(period, group):\n",
    "    tweets = group['Tweet'].tolist()\n",
    "    if not tweets:  \n",
    "        return period, ig.Graph(directed=False)  \n",
    "    \n",
    "    try:\n",
    "        graph = combined_igraph_each_period(tweets)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de la période {period}: {e}\")\n",
    "        graph = ig.Graph(directed=False)  \n",
    "    return period, graph\n",
    "\n",
    "\n",
    "def extract_igraph_features(ig_graph):\n",
    "    if ig_graph.vcount() == 0 or ig_graph.ecount() == 0:  # if empty graph\n",
    "        return {\n",
    "            \"average_degree\": 0,\n",
    "            \"degree_std\": 0,\n",
    "            \"density\": 0,\n",
    "            \"diameter\": 0,\n",
    "            \"average_clustering\": 0,\n",
    "            \"num_connected_components\": 0,\n",
    "        }\n",
    "\n",
    "    degrees = ig_graph.degree()\n",
    "    \n",
    "    clustering = ig_graph.transitivity_local_undirected(vertices=None)\n",
    "    average_clustering = np.mean([c for c in clustering if not np.isnan(c)]) if clustering else 0\n",
    "\n",
    "    features = {\n",
    "        \"average_degree\": np.mean(degrees),\n",
    "        \"degree_std\": np.std(degrees),\n",
    "        \"density\": ig_graph.density(),\n",
    "        \"diameter\": ig_graph.diameter() if ig_graph.is_connected() else 0,\n",
    "        \"average_clustering\": average_clustering,\n",
    "        \"num_connected_components\": len(ig_graph.clusters()),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'] / 1000, unit='s')  # Diviser par 1000 pour convertir en secondes\n",
    "\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(\"Début du prétraitement des tweets...\")\n",
    "df['Tweet'] = df['Tweet'].apply(preprocess_text)\n",
    "print(\"Prétraitement terminé.\")\n",
    "\n",
    "\n",
    "\n",
    "periods = df.groupby(pd.Grouper(key='Timestamp', freq='min'))\n",
    "igraph_dict = dict(\n",
    "    Parallel(n_jobs=-1)(\n",
    "        delayed(process_period_with_label)(period, group) for period, group in periods\n",
    "    )\n",
    ")\n",
    "graph_features = {period: extract_igraph_features(ig_graph) for period, ig_graph in igraph_dict.items()}\n",
    "features_df = pd.DataFrame.from_dict(graph_features, orient=\"index\")\n",
    "features_df['EventType'] = df.groupby(pd.Grouper(key='Timestamp', freq='min'))['EventType'].first()\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "X = features_df.drop(columns=['EventType']).values\n",
    "y = features_df['EventType'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#preditions\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy method1: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    val_df['Timestamp'] = pd.to_datetime(val_df['Timestamp'] / 1000, unit='s')  # Conversion du timestamp\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    periods = val_df.groupby(pd.Grouper(key='Timestamp', freq='min'))\n",
    "\n",
    "    igraph_dict_eval = dict(\n",
    "        Parallel(n_jobs=-1)(\n",
    "            delayed(process_period_with_label)(period, group) for period, group in periods\n",
    "        )\n",
    "    )\n",
    "\n",
    "    graph_features_eval = {\n",
    "        period: extract_igraph_features(ig_graph)\n",
    "        for period, ig_graph in igraph_dict_eval.items()\n",
    "    }\n",
    "\n",
    "    features_eval_df = pd.DataFrame.from_dict(graph_features_eval, orient=\"index\")\n",
    "    features_eval_df['ID'] = val_df.groupby(pd.Grouper(key='Timestamp', freq='min'))['ID'].first()\n",
    "\n",
    "    # Suppression des NaN\n",
    "    features_eval_df = features_eval_df.dropna()\n",
    "\n",
    "    X_eval = features_eval_df.drop(columns=['ID']).values\n",
    "\n",
    "    preds = clf.predict(X_eval)\n",
    "    dummy_preds = dummy_clf.predict(X_eval)\n",
    "\n",
    "    features_eval_df['EventType'] = preds\n",
    "    features_eval_df['DummyEventType'] = dummy_preds\n",
    "\n",
    "    predictions.append(features_eval_df[['ID', 'EventType']])\n",
    "    dummy_predictions.append(features_eval_df[['ID', 'DummyEventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('graph_predictions.csv', index=False)\n",
    "\n",
    "dummy_pred_df = pd.concat(dummy_predictions)\n",
    "dummy_pred_df.to_csv('dummy_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
